{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we create a model and training loop to classify food images from the Food101 into different categories. We start by importing the packages we need below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rampoddar/.pyenv/versions/miniforge3-4.10.3-10/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer \n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List files in the data\n",
    "def show_classes(data_path):\n",
    "    \"\"\"\n",
    "    Return class list\n",
    "    \"\"\"\n",
    "    return os.listdir(data_path)\n",
    "\n",
    "\n",
    "\n",
    "def display_random_image(data_path, class_name):\n",
    "    \"\"\"\n",
    "    Display a few randomly\n",
    "    picked images from training\n",
    "    data\n",
    "    \"\"\"\n",
    "    class_path = data_path / class_name\n",
    "    img_paths = [class_path / f for f in os.listdir(class_path)]\n",
    "    \n",
    "    n_images = len(img_paths)\n",
    "    image_id = np.random.randint(0, n_images)\n",
    "    \n",
    "    img_path = img_paths[image_id]\n",
    "    img = Image.open(img_path)\n",
    "    img.show()\n",
    "\n",
    "\n",
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths. \n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "def test_transform(train_data_path, class_name):\n",
    "    \"\"\"\n",
    "    Test transforms\n",
    "    \"\"\"\n",
    "    image_path_list = [] \n",
    "\n",
    "    for f in os.listdir(train_data_path / class_name):\n",
    "        image_path_list.append(train_data_path / class_name / f)\n",
    "\n",
    "    plot_transformed_images(image_path_list, \n",
    "                            transform=data_transform, n=3)\n",
    "\n",
    "\n",
    "def test_forward_pass(model, train_dataloader):\n",
    "    \"\"\"\n",
    "    Test forward pass\n",
    "    with a single image\n",
    "    \"\"\"\n",
    "    img_batch, label_batch = next(iter(train_dataloader))\n",
    "    img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred = model(img_single.to(device))\n",
    "\n",
    "    print(f\"Output logits:\\n{pred}\\n\")\n",
    "    print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "    print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "    print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*16*16,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # leverage the benefits of operator fusion\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          learning_rate: np.float64,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \"\"\"\n",
    "    Main training loop\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(params=model_0.parameters(), \n",
    "                                 lr=learning_rate)\n",
    "    \n",
    "    config = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"TinyVGG\",\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"food-vision\", config=config)\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        wandb.log({\"train_acc\": train_acc, \n",
    "                   \"train_loss\": train_loss, \n",
    "                   \"test_loss\": test_loss, \n",
    "                   \"test_acc\": test_acc})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshatgoel92\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rampoddar/Documents/code/torch-learn/wandb/run-20230402_120839-twekfbca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/akshatgoel92/food-vision/runs/twekfbca' target=\"_blank\">efficient-pond-1</a></strong> to <a href='https://wandb.ai/akshatgoel92/food-vision' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/akshatgoel92/food-vision' target=\"_blank\">https://wandb.ai/akshatgoel92/food-vision</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/akshatgoel92/food-vision/runs/twekfbca' target=\"_blank\">https://wandb.ai/akshatgoel92/food-vision/runs/twekfbca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [07:08<00:00, 85.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▅▁▃▆█</td></tr><tr><td>test_loss</td><td>███▇▁</td></tr><tr><td>train_acc</td><td>▂▁▂▄█</td></tr><tr><td>train_loss</td><td>█▆▆▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>0.50667</td></tr><tr><td>test_loss</td><td>1.00462</td></tr><tr><td>train_acc</td><td>0.50222</td></tr><tr><td>train_loss</td><td>1.04941</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-pond-1</strong> at: <a href='https://wandb.ai/akshatgoel92/food-vision/runs/twekfbca' target=\"_blank\">https://wandb.ai/akshatgoel92/food-vision/runs/twekfbca</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230402_120839-twekfbca/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 441.612 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "      # Set up device agnostic code\n",
    "      device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "      print(device)\n",
    "\n",
    "      # Check version\n",
    "      print(torch.__version__)\n",
    "      \n",
    "      train_data_path = Path('data') / 'train'\n",
    "\n",
    "      test_data_path = Path('data') / 'test'\n",
    "\n",
    "      class_name = 'pizza'\n",
    "\n",
    "      # Set random seeds\n",
    "      torch.manual_seed(42) \n",
    "\n",
    "      # Set random GPU seed\n",
    "      torch.cuda.manual_seed(42)\n",
    "\n",
    "      # Set number of epochs\n",
    "      NUM_EPOCHS = 5\n",
    "\n",
    "      learning_rate = 0.001\n",
    "\n",
    "\n",
    "      data_transform = transforms.Compose([transforms.Resize(size=(64, 64)),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "      show_classes(train_data_path)\n",
    "\n",
    "      display_random_image(train_data_path, class_name)\n",
    "\n",
    "\n",
    "      train_data = datasets.ImageFolder(root=train_data_path, \n",
    "                                    transform=data_transform, \n",
    "                                    target_transform=None)\n",
    "\n",
    "\n",
    "      test_data = datasets.ImageFolder(root=test_data_path, \n",
    "                                    transform=data_transform, \n",
    "                                    target_transform=None)\n",
    "\n",
    "\n",
    "      train_dataloader = DataLoader(dataset=train_data,\n",
    "                                    batch_size=1,\n",
    "                                    num_workers=os.cpu_count(),\n",
    "                                    shuffle=True)\n",
    "\n",
    "\n",
    "      test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=1,\n",
    "                              num_workers=os.cpu_count(),\n",
    "                              shuffle=True)\n",
    "\n",
    "\n",
    "      # Recreate an instance of TinyVGG\n",
    "      model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                        hidden_units=10, \n",
    "                        output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "\n",
    "      # Setup loss function and optimizer\n",
    "      loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "      start_time = timer()\n",
    "\n",
    "      # Train model_0 \n",
    "      train(model=model_0, \n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_fn=loss_fn, \n",
    "            epochs=NUM_EPOCHS)\n",
    "\n",
    "      # End the timer and print out how long it took\n",
    "      end_time = timer()\n",
    "\n",
    "      print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
